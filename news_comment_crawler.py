# -*- coding: utf-8 -*-
"""news_comment_crawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYBAOQ6H-52M2IDGOnZ_na46RJCOmiqN
"""

# 1. 패키지 및 자바 설치
!apt-get update # apt install을 올바르게 실해하기 위해 우분투를 업데이트
!apt-get install g++ openjdk-8-jdk python-dev python3-dev #라이센스 없는 Open JDK 설치
!pip3 install JPype1-py3 #JPype1-py3 설치
!JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64" #자바 경로 설정

!pip install --upgrade Beautifulsoup4 selenium #크롤링용 패키치
!pip3 install konlpy # 한국어 형태소 분석 패키지 설치
!pip install wordcloud #단어구름 만드는 패키지 설치
!pip install pyLDAvis  #LDA 시각화
!pip install fake_useragent

!pip install mglearn


#matplotlib 한글 폰트 설정 
import matplotlib as mpl
import matplotlib.pyplot as plt
 
!apt -qq -y install fonts-nanum
 
import matplotlib.font_manager as fm
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
font = fm.FontProperties(fname=fontpath, size=9)
plt.rc('font', family='NanumBarunGothic') 
mpl.font_manager._rebuild()

# 3. jpype 정상실행 확인
import jpype
print(jpype.isJVMStarted()) #return False:not running or 0:running

# 4. KoNLPy 불러오기
import konlpy

from konlpy.tag import Kkma
from konlpy.tag import Okt

# Kkma, Komoran, Hannanum, Okt, Mecab : KoNLPy에 내장된 형태소 분석/품사 태깅 클래스

from konlpy.utils import pprint


try:
  from bs4 import BeautifulSoup
except:
  !pip install --upgrade Beautifulsoup4
  from bs4 import BeautifulSoup

try:
  import selenium
except:
  !pip install --upgrade selenium
  import selenium

!pip install schedule
!pip install tqdm
import pandas as pd 
import numpy as np

from selenium import webdriver
import datetime
import requests
import time
from time import sleep
import re #정규식
import math #반올림
from tqdm import trange #
import os #파일 및 폴더 관리
import shutil #파일 한번에 삭제
import gensim #LDA
import gensim.corpora as corpora
import json
from fake_useragent import UserAgent
from multiprocessing import Pool
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import urllib.request  
import sys
from random import uniform



try:
  import GetOldTweets3 as got
except:
  !pip install GetOldTweets3
  import GetOldTweets3 as got




!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

import pandas as pd
aaa = pd.read_csv('/content/drive/My Drive/Crawling/naver_comment_간호장교_2020-04-07_2020-04-13/score.csv')
#aaa.sort_values(by = 'neg', ascending = False)[:40]
aaa.loc[972, 'url']



import matplotlib.font_manager as fm
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
font = fm.FontProperties(fname=fontpath, size=9)
plt.rc('font', family='NanumBarunGothic') 
mpl.font_manager._rebuild()

"""# 다음 url 추출, 댓글 추출"""



def daum_comment_crawler(query, s_date, e_date):
  url_list = get_url_daum_news(query, s_date, e_date)
  comments = reply_getter_daum(url_list)
  comments.to_csv('daum_comments_{}.csv'.format(query),index=False)
  return(comments)

def get_url_daum_news(query, s_date, e_date):
#ex. get_url_daum(query = '코로나', s_date = '20200217230014', e_date = '20200317230014')

#1. 탐색할 페이지 수 결정하기
  url_search_result = 'https://search.daum.net/search?w=news&sort=recency&q='+query+'&cluster=n&DA=PGD&dc=STC&pg=1&r=1&p=1&rc=1&at=more&sd='+s_date+'&ed='+ e_date + '&period=u'
    #1.3.2. 다음 기사 개수
  response = requests.get(url_search_result)
  html = response.text
  soup = BeautifulSoup(html, 'html.parser')
  
  total_news = soup.find('span', 'txt_info') # 검색 결과 개수 가져오기
  total_news = re.split(' / ', total_news.text)[1][0:-1] # 1-10 / 629건 과 같은 형식에서 629만 가져오기
  total_news = total_news.replace('약 ','')
  total_news = int(total_news.replace(',',''))

  print('total news:', total_news)
  total_pages = math.ceil(total_news/10)   
  print('pages:', total_pages)

  
#1.4.2. 다음 링크 추출
  links = []
  for i in range(1, total_pages+1):
    url_search_result = 'https://search.daum.net/search?w=news&sort=recency&q={}&cluster=n&DA=PGD&dc=STC&pg=1&r=1&p={}&rc=1&at=more&sd={}&ed={}&period=u'.format(query, page, s_date, e_date)
    response = requests.get(url_search_result)
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')   

    for atag in soup.find_all('a', {'class' : 'f_nb'}):
      links.append(atag['href'])
    
    if i%10 ==0:
      print(i-9, '-', i,  '번째 페이지의 기사 링크 모두 가져옴.')

  print('총', total_news, "개의 기사 중 'DAUM  뉴스'", len(links), '개 가져옴.')

  links = pd.DataFrame(links)
  links.columns = ['URLs']
  return links



def reply_getter_daum(URL_list):
  
  # 뽑아낸 링크에서 댓글 추출
  browser = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
  df = pd.DataFrame(columns=['press', 'title', 'time', 'URL', 'replies'])

  for i in URL_list.index:
    rep_url = URL_list.loc[i, 'URLs']
    browser.implicitly_wait(5) #웹 드라이버
    browser.get(rep_url)

    #더보기 계속 클릭하기
    while True:
      try:
        see_more_button = browser.find_element_by_css_selector("#alex-area > div > div > div > div.cmt_box > div.alex_more > a")
        see_more_button.click()        
        time.sleep(3)
      except:
        break

    html = browser.page_source
    bs = BeautifulSoup(html, 'lxml')

    press = bs.find('div', {'class' : 'head_view'})('img')[0]['alt'] # 언론사
    title = bs.find("h3", "tit_view").text #제목
    time  = bs.find_all("span", {'class' : 'txt_info'})[1].text[3:] #작성시간
    comments = bs.find_all("p", "desc_txt font_size_17") #댓글
    for comment in comments:
      reply = comment.text.replace('\n','')
      df = df.append(pd.DataFrame([[press, title, time, rep_url, reply]], columns=['press', 'title', 'time', 'URL', 'replies']), ignore_index=True)
      #print('댓글 하나 추출')

     
    print(i + 1, '번째 기사의 댓글을 모두 추출했습니다.')
    
  browser.quit()
  df.to_csv('/content/drive/My Drive/Crawling/daum_replies.csv',header=True, index=False)
  print('댓글 추출 완료')
  return(df)

# 일별 블로그 개수 가져오는 함수
def how_many_naver_blogs(query, s_date, e_date):
  dt_index = pd.date_range(start = s_date, end = e_date)
  dt_list = dt_index.strftime("%Y%m%d").tolist()
  #print(dt_list)
  how_many = {}
  for dt in dt_list:
    base_url = 'https://search.naver.com/search.naver'
    d = {'nso' : 'so%3Add%2Cp%3Afrom{}to{}'.format(s_date, e_date), 'post_blogurl' : 'blog.naver.com' ,'sm':'tab_pge', 'date_option' : 8, 'dup_remove' : 1, 'post_blogurl_without' : '', 'srchby' : 'all', 'st' : 'date', 'where' : 'post'}
    d['query'] = query
    d['date_from'] = dt
    d['date_to'] = dt
    d['start'] = 1
  
    # 총 블로그 개수 확인
    response = requests.get(base_url, params=d)
    soup = BeautifulSoup(response.text, 'lxml')
    tot = soup.select( '#main_pack > div.blog.section._blogBase._prs_blg > div > span' )[0] # 검색 결과 개수 가져오기
    tot = int(tot.text.split(' / ')[1][:-1].replace(',', '')) 
    how_many[dt] = tot
  return how_many

#테스트
dt_index = pd.date_range(start = '2020-04-06', end = '2020-04-10')
dt_list = dt_index.strftime("%Y%m%d").tolist()
  
how_many = {}
url = 'https://search.naver.com/search.naver?nso=so%253Add%252Cp%253Afrom20200406to20200413&post_blogurl=blog.naver.com&sm=tab_pge&date_option=8&dup_remove=1&post_blogurl_without=&srchby=all&st=date&where=post&query=%EA%B0%84%ED%98%B8%EC%82%AC%EA%B4%80&date_from=20200406&date_to=20200413&start=1'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
# 총 블로그 개수 확인
litags = soup.select('ul.type01 > li') #ok
for litag in litags:
  item = litag.select('dl > dt > a')[0]
  title = item.text
  url = item['href']   
  print(url)
      #df = df.append(pd.DataFrame([[press, title, link]], columns=['press','title','URL']), ignore_index=True)

 # return how_many

"""# 다음 블로그"""

def daum_blog_crawler(query, s_date, e_date):
  url_list = get_url_daum_blog(query, s_date, e_date)
  blogs = blog_getter_daum(url_list)
  blogs.to_csv('daum_blogs_{}.csv'.format(query),index=False)
  return(blogs)

def get_url_daum_blog(query, s_date, e_date):
    url_search_result = 'https://search.daum.net/search?nil_suggest=btn&w=blog&q={}&f=section&SA=daumsec&sd={}&ed={}&period=u&page=1&sort=recency&DA=PGD'.format(query, s_date, e_date)
    response = requests.get(url_search_result)
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
      
    total_blogs = soup.find('span', 'txt_info') # 검색 결과 개수 가져오기
    total_blogs = re.split(' / ', total_blogs.text)[1][0:-1] # 1-10 / 629건 과 같은 형식에서 629만 가져오기
    total_blogs = total_blogs.replace('약 ','')
    total_blogs = int(total_blogs.replace(',',''))

    print('total blogs:', total_blogs)
    total_pages = math.ceil(total_blogs/10)   
    print('pages:', total_pages)

    #1.4.2. 다음 링크 추출
    links = []
    titles = []
    for i in range(total_pages):
        url_search_result = 'https://search.daum.net/search?nil_suggest=btn&w=blog&q={}&f=section&SA=daumsec&sd={}&ed={}&period=u&page={}&sort=recency&DA=PGD'.format(query, s_date, e_date, i+1)
        response = requests.get(url_search_result)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')   

        for atag in soup.find_all('a', {'class' : 'f_link_b'}):
            links.append(atag['href'])
            titles.append(atag.text)
      #print(i+1)


    df = pd.DataFrame.from_items( [('title', titles), ('url', links)] )
    print('총 {}개의 DAUM 블로그 중 {}개 가져옴'.format(total_blogs, len(links)))

    #links = pd.DataFrame(links)
    #links.columns = ['URLs']
    return(df)

# 블로그 본문 내용 긁어오는 함수 
def blog_getter_daum(URL_list): #못 긁어오는 포스트 몇개 존재
  df = pd.DataFrame( columns=['title', 'URL', 'content'] ) 
  for i in URL_list.index:
    blog_url = 'http://m.' + URL_list.loc[i, 'url'][7:]
    blog_title = URL_list.loc[i, 'title']

    headers ={'User-Agent': 'Mozilla/5.0'} # 403 forbidden 오류를 피하기 위해
    response = requests.get(blog_url, headers = headers)
    #print('{}번째 블로그 접속 성공'.format(i+1, response.status_code))
    html = response.text
    bs = BeautifulSoup(html, 'html.parser')


    text_divs1 = bs.find('div', {'class':"blogview_content useless_p_margin"})
    text_divs2 = bs.find('div', {'id':"article"})
    text_divs3 = bs.find('div', {'class':'article_body'})

    if isinstance(text_divs1 , type(None)):
      if isinstance(text_divs2 , type(None)):
        if not isinstance(text_divs3 , type(None)):
          final_text_div = text_divs3
          #print('유형3')
      else:
        final_text_div = text_divs2
        #print('유형2')
    else:
      if isinstance(text_divs2 , type(None)) and isinstance(text_divs3 , type(None)):
          final_text_div = text_divs1
          #print('유형1')


    blog_content = ''
    for text in final_text_div:
      text = re.sub(r'(\<.+?\>)', ' ', str(text))
      if text not in blog_content:
        blog_content += text
        
    blog_content = blog_content.split('관련 태그 목록')[0]
    blog_content = blog_content.replace('\n', '')
    blog_content = blog_content.replace('\xa0', '').strip()
    blog_content = re.sub(' +', ' ', blog_content)

    df = df.append(pd.DataFrame([[blog_title, blog_url, blog_content]], columns = ['title', 'URL', 'content']), ignore_index=True)
    if i%10 == 0:
      print('{}번째 블로그 완료'.format(i+1))
  return(df)

"""# 트위터"""

################################################################################
# 가져올 범위를 정의
# 예제 : 2019-04-21 ~ 2019-04-24

import time
import datetime
from random import uniform
from tqdm import tqdm_notebook

s_date = "2020-04-07"
e_date = "2020-04-13"
query = '간호장교'

dt_index = pd.date_range(start = s_date, end = e_date)
days_range = dt_index.strftime("%Y-%m-%d").tolist()

print("=== 설정된 트윗 수집 기간은 {} 에서 {} 까지 입니다 ===".format(days_range[0], days_range[-1]))
print("=== 총 {}일 간의 데이터 수집 ===".format(len(days_range)))
################################################################################
# 특정 검색어가 포함된 트윗 검색하기 (quary search)


# 수집 기간 맞추기
start_date = days_range[0]
end_date = (datetime.datetime.strptime(days_range[-1], "%Y-%m-%d") 
            + datetime.timedelta(days=1)).strftime("%Y-%m-%d") # setUntil이 끝을 포함하지 않으므로, day + 1

# 트윗 수집 기준 정의
tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\
                                           .setSince(start_date)\
                                           .setUntil(end_date)\
                                           .setMaxTweets(-1)

# 수집 with GetOldTweet3
print("수집 시작.. from {} to {}".format(days_range[0], days_range[-1]))
start_time = time.time()

tweet = got.manager.TweetMana료er.getTweets(tweetCriteria)

print("수집 종료.. {0:0.2f} Minutes".format((time.time() - start_time)/60))
print("=== Total num of tweets is {} ===".format(len(tweet)))
################################################################################
# 원하는 변수 골라서 저장하기

# initialize
tweet_list = []

for index in tweet: #위에서 가져온 정보 정리
    
    # 메타데이터 목록 
    username = index.username
    link = index.permalink 
    content = index.text
    tweet_date = index.date.strftime("%Y-%m-%d")
    tweet_time = index.date.strftime("%H:%M:%S")
    retweets = index.retweets
    favorites = index.favorites
    
    # ===beautifulsoap로 유저 정보 수집 시작 ===
    try:
        personal_link = 'https://twitter.com/' + username
        response = requests.get(personal_link)
        bs_obj = BeautifulSoup(response.text, 'html.parser')  
        #bs_obj = get_bs_obj(personal_link)
        uls = bs_obj.find("ul", {"class": "ProfileNav-list"}).find_all("li")
        div = bs_obj.find("div", {"class": "ProfileHeaderCard-joinDate"}).find_all("span")[1]["title"]


        # 가입일, 전체 트윗 수, 팔로잉 수, 팔로워 수
        joined_date = div.split('-')[1].strip()
        num_tweets = uls[0].find("span", {"class": "ProfileNav-value"}).text.strip()
        num_following = uls[1].find("span", {"class": "ProfileNav-value"}).text.strip()
        num_follower = uls[2].find("span", {"class": "ProfileNav-value"}).text.strip()
    
    except AttributeError:
        print("=== Attribute error occurs at {} ===".format(link))
        print("link : {}".format(personal_link))   
        pass
     
    # 결과 합치기
    info_list = [tweet_date, tweet_time, username, content, link, retweets, favorites, 
                 joined_date, num_tweets, num_following, num_follower]
    tweet_list.append(info_list)
    
    # 휴식. 너무 자주 긁어오면 차단당함
    time.sleep(uniform(1,2))
################################################################################
# 파일 저장하기
dir_path = '/content/drive/My Drive/Crawling'
dir_name = 'tweet_{}_{}_{}'.format(query, s_date, e_date)
os.mkdir(dir_path + '/' + dir_name)

twitter_df = pd.DataFrame(tweet_list, 
                          columns = ["time", "time2", "user_name", "content", "URL", "retweet_counts", "favorite_counts",
                                    "user_created", "user_tweets", "user_followings", "user_followers"])

# csv 파일 만들기
twitter_df.to_csv(dir_path + '/' + dir_name + "/sample_twitter_data_{}_to_{}.csv".format(days_range[0], days_range[-1]), index=False)
print("=== {} tweets are successfully saved ===".format(len(tweet_list)))

"""# 그래프"""



fig, ax = plt.subplots(2,4, figsize=(15,15))
for x in range(7):

  a = int(x/4) 
  b = x % 4 
  pos_rate = score_per_day.iloc[x, 4]
  neg_rate = score_per_day.iloc[x, 5]
  neu_rate = 1 - pos_rate - neg_rate
  ratio = [pos_rate, neg_rate, neu_rate]
  ratio_list.append(ratio)

  ax[a, b].pie(ratio, labels=labels, startangle=90)
  daystr = str(int(score_per_day.index[x]))
  ax[a, b].title.set_text('-'.join( [daystr[:4] , daystr[4:6], daystr[6:]]) )
  
ax[1,3].pie(tot_ratio, labels=labels, startangle=90)
ax[1, 3].title.set_text('7일 전체')

"""# 토큰화 + 스코어링"""

path = '/content/drive/My Drive/Crawling/naver_comment_간호장교_2020-04-07_2020-04-13'
token_new = tokenizer(path)
c_score1, c_score2 = scorer(path)

path = '/content/drive/My Drive/Crawling/naver_blog_간호장교_2020-04-07_2020-04-13'
token_new = tokenizer(path)
b_score1, b_score2 = scorer(path)

path = '/content/drive/My Drive/Crawling/tweet_간호장교_2020-04-07_2020-04-13'
token_new = tokenizer(path)
t_score1, t_score2, t_tot_ratio = scorer(path)

def tokenizer(path):
  # 품사 태깅

  filepath = path  + '/' +  'total.csv'
  dfdf = pd.read_csv(filepath)
  contents = dfdf['content']
  n = len(contents) 
  print(n, '원본 테이블')
  
  pos_so_far = []
  for content in contents:
    if type(content) == str:
      pos_so_far.append( konlpy.tag.Okt().pos(content) )
    else:
      pos_so_far.append([])

  print(len(pos_so_far), '품사 태깅')
  ################################################################################
  # 뺄 단어 리스트 만들기.
  stop_list_so_far = []
  #stopwords 뽑기
  for sentence in pos_so_far:
    for word in sentence:
      if word[1] not in ['Noun']:
        stop_list_so_far.append(word[0])
  stop_list = list(set(stop_list_so_far))
  with open('/content/drive/My Drive/Crawling/korean_stopwords.txt', 'r') as korstop:
    for word in korstop.readlines():
      stop_list.append(word.strip())

  print(len(stop_list), 'stopwords')
  ################################################################################
  # stopword 써서 토큰화

  token_so_far = []
  for content in contents:
    if type(content) == str:
      token_so_far.append( konlpy.tag.Okt().morphs(content) )
    else:
      token_so_far.append([])
  
  print(len(token_so_far), '토큰')
  token_new = []
  for line in token_so_far:
    new_line = []
    for word in line:
      if (len(word) > 1) & (word not in stop_list): #한 글자 단어와 stopword는 다 제거
        new_line.append(word)
    token_new.append(new_line)
  print(len(token_new), '최종')
  tokens = json.dumps(token_new)
  with open(path + '/' + 'token.json', 'w') as fileref:
    fileref.write(tokens)

  return token_new

################################################################################
################################################################################

def scorer(path):
  with open('/content/drive/My Drive/Crawling/SentiWord_info.json', 'r') as fileref:
    dict_str = fileref.read() 
  sent_dict = json.loads(dict_str) #감성어 사전

  with open( path + '/token.json', 'r') as fileref:
    my_words = json.loads(fileref.read()) #토큰파일

  dfdf = pd.read_csv(path + '/' + 'total.csv') #댓글 파일
  ################################################################################
  #점수 계산
  scorelist = df = pd.DataFrame(np.nan, index = np.arange(len(my_words)), columns=['pos', 'neu', 'neg', 'in_dict', 'out_dict'])
  counter = 0

  for i in range(len(my_words)):
    #print(i, '번째 텍스트에 점수 부여')
    sentence = my_words[i]
    scorelist.loc[i,['pos','neu','neg','in_dict','out_dict']] = 0
    
    score = {'pos':0, 'neu':0, 'neg':0, 'in_dict':0}
    for word in sentence:
      #print(word)
      counter = 0
      for sentword in sent_dict:
        if word in [ sentword['word'], sentword['word_root'] ] and counter == 0:
          counter = 1
          pol = int(sentword['polarity'] )
          if pol > 0:
            score['pos']+=1
          elif pol == 0:
            score['neu']+=1
          else:
            score['neg']+=1
    score['in_dict'] = sum(score.values())
    scorelist.loc[i, ['pos', 'neu', 'neg', 'in_dict', 'out_dict']] = [score['pos'], score['neu'], score['neg'], score['in_dict'], len(sentence) - score['in_dict']]
  ################################################################################
  # rate 계산
  scorelist[ 'pos_rate' ] = scorelist['pos'] / scorelist['in_dict']
  scorelist[ 'neg_rate' ] = scorelist['neg'] / scorelist['in_dict']
  ################################################################################
  #테이블 join
  final = dfdf.join(scorelist, how='left')
  final.to_csv(path + '/' + 'score.csv', index = False)
  ################################################################################
  #일별 합계
  score_per_day = final.groupby('time')[ ['pos', 'neu', 'neg', 'in_dict'] ].sum()
  score_per_day['pos_rate'] = score_per_day['pos'] / score_per_day['in_dict']
  score_per_day['neg_rate'] = score_per_day['neg'] / score_per_day['in_dict']
  score_per_day.to_csv(path + '/' + 'score_per_day.csv')
  ################################################################################
  ## 전체 합계
  tot_ratio= [ final['pos'].sum(), final['neg'].sum(), final['neu'].sum()]
 
  
  
  return final, score_per_day, tot_ratio

import matplotlib.pyplot as plt
import matplotlib as mpl
!apt -qq -y install fonts-nanum

import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
from matplotlib import style
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
import matplotlib.font_manager as fm
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
font = fm.FontProperties(fname=fontpath, size=9)
plt.rc('font', family='NanumBarunGothic') 
mpl.font_manager._rebuild()
style.use('ggplot')
import datetime

import pandas as pd
s_date = "2020-04-07"
e_date = "2020-04-13"
query = '간호장교'

dt_index = pd.date_range(start = s_date, end = e_date)
days_range = dt_index.strftime("%Y-%m-%d").tolist()
days_range2 = dt_index.strftime("%Y%m%d").tolist()

import pd
t_score1 = pd.read_csv('/content/drive/My Drive/Crawling/tweet_간호장교_2020-04-07_2020-04-13/score.csv')
t_score2 = pd.read_csv('/content/drive/My Drive/Crawling/tweet_간호장교_2020-04-07_2020-04-13/score_per_day.csv')

c_score1.groupby('time')[ 'content' ].count()

b_score2

################################################################################
#댓글이 없는 날은 아예 행이 없음.
#날짜로 접근하는 방법을 써야 함
#그래프 그리기
def draw_pie(final, score_per_day):
  fig, ax = plt.subplots(2,4, figsize=(15,15))

  labels = ['긍정', '부정', '중립']
  i = 0
  j = 0
  for day in days_range2:
    a = int(i/4) 
    b = i % 4 

    if day in score_per_day.index:
      pos_rate = score_per_day['pos_rate'].iloc[j]
      neg_rate = score_per_day['neg_rate'].iloc[j]
      neu_rate = 1 - pos_rate - neg_rate
      ratio = [pos_rate, neg_rate, neu_rate]
      print(ratio)
      if sum(ratio) >0:
        ax[a, b].pie(ratio, labels=labels, startangle=90, autopct='%1.1f%%')
        ax[a, b].title.set_text( score_per_day.index[j] )
        j += 1
      else:
        ax[a, b].title.set_text( score_per_day.index[j] + ' 없음' )
        j += 1
      #daystr = str(int(score_per_day.index[j]))
      
      #ax[a, b].title.set_text('-'.join( [daystr[:4] , daystr[4:6], daystr[6:]]) )

    else:
      ax[a, b].title.set_text(day + ' 없음' )

    i += 1
  tot_ratio= [ final['pos'].sum(), final['neg'].sum(), final['neu'].sum()]
  ax[1, 3].pie(tot_ratio, labels=labels, startangle=90, autopct='%1.1f%%')
  ax[1, 3].title.set_text('7일 전체')

draw_pie(b_score1, b_score2)

t_score1.sort_values(by = 'pos_rate', ascending = False).iloc[0,3]

'2020-04-07' in t_score2.index

t_score2

scorelist = pd.read_csv('/content/drive/My Drive/Crawling/naver_blog_간호사관_2020-04-07_2020-04-13/sent_scores.csv')

c_score1

# Create Dictionary
id2word = corpora.Dictionary(token_new)
  
  # Create Corpus
texts = token_new

  # Term Document Frequency
corpus = [id2word.doc2bow(text) for text in token_new]
  
  # View
  #print(corpus[:1])

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
id2word=id2word,
num_topics= 5,
random_state = 100,
update_every=1,
chunksize=100,
passes=10,
alpha='auto',
per_word_topics=True)

pprint(lda_model.print_topics())

topic_0 = '0.008*"대표" + 0.006*"마스크" + 0.006*"국민" + 0.006*"대통령" + 0.006*"경제" + 0.006*"선거" + 0.006*"후보" + 0.006*"사태" + 0.006*"개학" + 0.005*"감염"'
topic_1 = '0.012*"생각" + 0.009*"있는" + 0.005*"사회" + 0.005*"합니다" + 0.005*"국가" + 0.005*"지금" + 0.004*"문제" + 0.004*"국민" + 0.004*"자유" + 0.003*"역사"'
topic_2 = '0.035*"대통령" + 0.026*"천안함" + 0.013*"문재인" + 0.013*"서해" + 0.012*"수호" + 0.010*"참석" + 0.009*"행사" + 0.009*"유족" + 0.008*"소행" + 0.008*"희생"'
topic_3 = '0.007*"상황" + 0.007*"남북" + 0.007*"문제" + 0.006*"평화" + 0.006*"있는" + 0.005*"대통령" + 0.005*"한반도" + 0.005*"세계" + 0.005*"대한" + 0.005*"관련"'
topic_4 = '0.014*"천안함" + 0.008*"해군" + 0.006*"사건" + 0.005*"호위함" + 0.005*"보고" + 0.004*"어뢰" + 0.004*"칼럼" + 0.003*"다시" + 0.003*"전차" + 0.003*"있는"'
topics = [topic_0, topic_1, topic_2, topic_3, topic_4]
def cleaner(topic_1):
  topic_1_spt = []
  for item in topic_1.split(' + '):
    item_new = item.split('*')[1][1:-1]
    topic_1_spt.append(item_new)
  return(topic_1_spt)

def sent_scorer(my_words):
  score = {'pos' : 0, 'neu' :0, 'neg':0}
  counter = 0
  for i in trange(len(my_words)):
    sentence = my_words[i]
    for word in sentence:
      counter = 0
      for sentword in sent_dict:
        if word in [ sentword['word'], sentword['word_root'] ] and counter == 0:
          counter = 1
          pol = int(sentword['polarity'] )
          if pol > 0:
            score['pos']+=1
          # print('positive', word)
          elif pol == 0:
            score['neu']+=1
            #print('neutral', word)
          else:
            score['neg']+=1
          #  print('negative', word)
  with open('/content/drive/My Drive/Crawling/naver_blog_천안함_2020-01-01_2020-04-04/score.json', 'w') as fileref:
    fileref.write(json.dumps(score))
  return score

for topic in topics:
  print(sent_scorer(cleaner(topic)))

print(lda_model.get_topic_terms)

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim # don't skip this
import matplotlib.pyplot as plt

vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
pyLDAvis.display(vis)

pyLDAvis.display(vis)

vis



LDA(filepath = '/content/drive/My Drive/Crawling/crawl_육군_2020-04-28_2020-04-28/NC/temp/NC_url_20200428_20200428.csv', num_topics = 10, random_state=42):

def LDA(filepath, num_topics, random_state):
  contents = pd.read_csv(filepath)['content']
  pos_so_far = []
  for content in contents:
    if type(content) == str:
      pos_so_far.append( konlpy.tag.Okt().pos(content) )

  stop_list_so_far = [] # 뺄 단어 리스트 만들기.
  for sentence in pos_so_far:
    for word in sentence:
      if word[1] not in ['Noun']:
        stop_list_so_far.append(word[0])
  stop_list = list(set(stop_list_so_far))
  with open('/content/drive/My Drive/Crawling/korean_stopwords.txt', 'r') as korstop:
    for word in korstop.readlines():
      stop_list.append(word.strip())

  print(len(stop_list))

  token_so_far = []
  for content in contents:
    if type(content) == str:
      token_so_far.append( konlpy.tag.Okt().morphs(content) )

  token_new = []
  for line in token_so_far:
    new_line = []
    for word in line:
      if (len(word) > 1) & (word not in stop_list):
        new_line.append(word)
    token_new.append(new_line)

path = '/content/drive/My Drive/Crawling/naver_blog_천안함_2020-01-01_2020-04-04/naver_blogs_total_2020-01-01_2020-04-04.csv'
lda_naver = LDA(path, 10, 200)

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim # don't skip this
import matplotlib.pyplot as plt

vis = pyLDAvis.gensim.prepare(lda_naver[0], lda_naver[1], lda_naver[2])
pyLDAvis.display(vis)

pyLDAvis.display(vis)

"""# 클래스"""

class Crawler():
  def __init__(self, query, s_date, e_date, dir, retry = False, sample = 0, cores = 2):
      self.query = query
      self.d_range0 = ( pd.date_range(start = s_date, end = e_date) ).strftime("%Y.%m.%d").tolist()
      self.d_range1 = ( pd.date_range(start = s_date, end = e_date) ).strftime("%Y%m%d").tolist()
      self.d_range2 = ( pd.date_range(start = s_date, end = e_date) ).strftime("%Y-%m-%d").tolist()
      self.sample = sample
      self.cores = cores
      self.retry = retry



      self.count_NC = {} # 일별 뉴스 카운터
      for d in self.d_range2:
        self.count_NC[d] = None

      self.count_NB = {} # 일별 블로그 카운터
      for d in self.d_range2:
        self.count_NB[d] = None

      
      self.path = dir + '/crawl_{}_{}_{}'.format(query, s_date, e_date) + '/' #공통 디렉토리
      
      self.NC_path = []
      self.NC_path.append( self.path + 'NC/' )
      self.NC_path.append( self.NC_path[0] + 'temp/' )
      self.NC_path.append( self.NC_path[1] +  'url/' )
      self.NC_path.append( self.NC_path[1] +  'data/' )
      
      self.NB_path = []
      self.NB_path.append( self.path + 'NB/' )
      self.NB_path.append( self.NB_path[0] + 'temp/' )
      self.NB_path.append( self.NB_path[1] +  'url/' )
      self.NB_path.append( self.NB_path[1] +  'data/' )

     


      self.T_path = self.path + 'T/'

      if not self.retry: #retry = False이면
        if os.path.isdir(self.path):
          shutil.rmtree(self.path)
        os.mkdir(self.path)
     
        
        
      
      

      #댓글을 위한 디렉토리 만들기
        os.mkdir(self.path + 'NC/')
        os.mkdir(self.NC_path[1])
        os.mkdir(self.NC_path[2])
        os.mkdir(self.NC_path[3])

        for path in self.NB_path:
          os.mkdir(path)
        


        os.mkdir(self.T_path)

    
      if self.retry and os.path.isfile( self.NB_path[1] + 'NB_url_{}_{}.csv'.format(self.d_range1[0], self.d_range1[-1]) ):
        self.NB_links = pd.read_csv( self.NB_path[1] + 'NB_url_{}_{}.csv'.format(self.d_range1[0], self.d_range1[-1]) )
    
################################################################################ 
  def delete(self):
    shutil.rmtree(self.path)     
      
################################################################################
  def getNClinks(self, i):
  #1. 탐색할 페이지 수 결정하기
    df = pd.DataFrame(columns=['press', 'title', 'url'])
    base_url = 'https://search.naver.com/search.naver'
    d = {'sort':1, 'photo':0, 'field':0, 'where':'news', 'reporter_article':'', 'pd':3, 'docid':'', 'refresh_start':0, 'mynews':0 }
    d['query'] = self.query
    d['ds'] = self.d_range0[i] #ex. '2020.03.01'
    d['de'] = self.d_range0[i]
    d['start'] = 1
    d['nso'] = 'so:dd,p:from{}to{},a:all'.format(self.d_range1[i], self.d_range1[i] )
    print(self.d_range0[i])
    response = requests.get(base_url, params=d)
   
    # 네이버 기사 개수 가져오기
    soup = BeautifulSoup(response.text, 'lxml')

    total_news = soup.find('div', 'title_desc') # 검색 결과 개수 가져오기
    if isinstance(total_news , type(None)):
      return None #검색 결과가 아예 없으면 여기서 끝. 임시파일도 생성x. 뉴스는 있는데 네이버 뉴스만 있으면 빈 임시파일이 생성됨.
    else:
      total_news = re.split(' / ', total_news.text)[1][0:-1] # 1-10 / 629건 과 같은 형식에서 629만 가져오기. '/'로 쪼갠 다음, '건'을 지운다.
      total_news = int(total_news.replace(',','')) # 나눗셈을 하기 위해 자릿수 표시하는 ','를 지우고, string을 int로 변경
      
      self.count_NC[ self.d_range2[i] ] = total_news #이 날의 뉴스 개수를 count_NC에 저장
      
      print('total news:', total_news) # 뉴스 개수 출력
      total_pages = math.ceil(total_news/10) # 검색 결과 페이지 수 계산. 이 개수만큼 for문을 돌려서 각 페이지의 뉴스기사 주소를 다 긁어올 것임.  
      print('pages:', total_pages) # 검색 결과 페이지 수 출력
    
      
  #2. 링크 추출
    sample_count = 0
    for j in range(total_pages):
      if (sample_count < self.sample) or self.sample ==0:
        d['start'] = str(j*10 + 1) # 1, 11, 21, 31, ...
        response = requests.get(base_url, params=d)
        soup = BeautifulSoup(response.text, 'lxml')
        
        litags = soup.select('ul.type01 > li')
        for litag in litags:
          is_naver = litag.select('dl > dd > a')
          if len(is_naver) > 0:
            link = litag.select('dl > dd > a')[0]['href']
            title = litag.select('dl > dt > a')[0]['title']
            press = litag.select('dl > dd > span')[0].text.replace('언론사 선정', '')
            df = df.append(pd.DataFrame([[press, title, link]], columns=['press','title','url']), ignore_index=True)
            sample_count += 1
      else:
        break
        if j%10 ==0:
          print('{}th page'.format(j))
        
    #링크 다 가져온 후 결과 출력
   # df = df.iloc[self.sample+1]
   
    
    df['time'] = self.d_range1[i] #i가 맞음
    df['v1'] = np.arange(len(df.index))
    df.drop_duplicates(['url'])
    df = df.drop('v1', axis = 1)

    if self.sample > 0 and len(df.index) > self.sample:
      df = df[:self.sample]
      
    df.to_csv( self.NC_path[2] +'naver_comment_url{}_{}.csv'.format(self.query, self.d_range1[i]),index=False) #i가 맞음
    print('총 {}개의 기사 중 {}개의 네이버 기사(댓글 달기 가능)를 가져옴'.format(total_news, len(df.index)))
    #self.NC_links[d_range1[i]] = df
 
################################################################################
  #pool 써서 url 목록 가져오는 함수
  def get_links(self, dtp): 
    print( len(self.d_range1), '일')

    if dtp == 'NC':
      n_iter = self.set_n_iter( self.d_range1, self.NC_path[2] )
      job = self.getNClinks #함수를 변수에 담을 수 있음
      path1 = self.NC_path[1]
      path2 = self.NC_path[2]
    if dtp == 'NB':
      n_iter = self.set_n_iter( self.d_range1, self.NB_path[2] )
      job = self.getNBlinks
      path1 = self.NB_path[1]
      path2 = self.NB_path[2]
    
    with Pool(processes = self.cores) as pool:
      pool.map( job, n_iter )      
    
    #합치기
    file_list = os.listdir( path2 )
    file_list.sort()
    for i in range( len(file_list) ): #빈 df는 base로 쓸 수가 없음
      filename = file_list[i]
      final = pd.read_csv(path2 +filename)
      if not final.empty:
        break
  
    for filename in file_list[ i+1 : ]:
      final = final.append( pd.read_csv( path2 +filename) , ignore_index=True ) #df는 파이썬 list와 다르게 append가 mutation이 아님
    
    final_filename = path1 + '{}_url_{}_{}.csv'.format( dtp, self.d_range1[0], self.d_range1[-1] )
    final.to_csv( final_filename, index=False)
    if os.path.isfile( final_filename ):
      shutil.rmtree( path2 )
    
    if dtp == 'NC':
      self.NC_links = final
    if dtp == 'NB':
      self.NB_links = final
################################################################################
  #네이버 뉴스 댓글 긁어오는 함수
  def getNCcomment(self, i):
     
    df = pd.DataFrame(columns=['content'])

    # 댓글은 '더 보기'를 여러 번 눌러야 다 볼 수 있으므로, 동적 크롤링을 위해 Selenium 사용
    browser = webdriver.Chrome('chromedriver',options=chrome_options) #크롬 브라우저 실행
    rep_url = '{}&m_view=1&includeAllCount=true&m_url=%2Fcomment%2Fall.nhn%3FserviceId%3Dnews%26gno%3Dnews417%2C0000512678%26sort%3Dlikability'.format(self.NC_links.iloc[i,2])
      
    browser.implicitly_wait(4) #웹 드라이버
    browser.get(rep_url)

    #더보기 계속 클릭하기
    while True:
      try:
        see_more_button = browser.find_element_by_css_selector('.u_cbox_page_more')
        see_more_button.click()        
        time.sleep(1)
      except:
        break
      
    #댓글추출
    contents = browser.find_elements_by_css_selector('span.u_cbox_contents')
    for content in contents:
      df = df.append(pd.DataFrame( [[content.text]], columns=['content'] ), ignore_index=True)
      #print(i + 1, '번째 댓글 추출') #결과 알림
      
    browser.quit() #브라우저 종료
    df['press'] = self.NC_links.iloc[i,0]
    df['title'] = self.NC_links.iloc[i,1]
    df['url'] = self.NC_links.iloc[i,2]
    df['time'] = self.NC_links.iloc[i,3]

    
    df['v1'] = np.arange(len(df.index))
    df.drop_duplicates(['content'])
    df.drop('v1', axis = 1)
    df.to_csv(self.NC_path[3]+'/naver_comment{}_{}.csv'.format(self.query, i),index=False)
################################################################################
  def get_content(self, dtp):
    print( len(self.d_range1), '일')
    
    if dtp == 'NC':
      with Pool(processes = self.cores) as pool:
        pool.map( self.getNCcomment, range( len(self.NC_links.index)) )
        path1 = self.NC_path[1]
        path3 = self.NC_path[3]
    if dtp == 'NB':
      with Pool(processes = self.cores) as pool:
        n_iter = self.set_n_iter( self.NB_links.index, self.NB_path[3] )
        pool.map( self.getNBcontent, n_iter )
        path1 = self.NB_path[1]
        path3 = self.NB_path[3]
    
    file_list = os.listdir( path3 )
    file_list.sort()

    for i in range( len(file_list) ): #빈 df는 base로 쓸 수가 없음
      filename = file_list[i]
      final = pd.read_csv(path3 +filename)
      if not final.empty:
        break
  
    for filename in file_list[ i+1 : ]:
      final = final.append( pd.read_csv( path3 +filename) , ignore_index=True )
    
    final_filename = path1 + '/{}_total_{}_{}.csv'.format( dtp, self.d_range1[0], self.d_range1[-1] )
    final.to_csv(final_filename ,index=False)
    if os.path.isfile(final_filename ):
      shutil.rmtree( path3 )

    if dtp == 'NC':
      self.NC_comments = final
    if dtp == 'NB':
      self.NB_contents = final
################################################################################   
  def getNBlinks(self, i): #for one day
  #1. 탐색할 페이지 수 결정하기
    print(self.d_range0[i])
    df = pd.DataFrame(columns=['title', 'url'])
    
    base_url = 'https://search.naver.com/search.naver'
    d = {'nso' : 'so%3Add%2Cp%3Afrom{}to{}'.format(self.d_range0[i], self.d_range0[i]), 'post_blogurl' : 'blog.naver.com' ,'sm':'tab_pge', 'date_option' : 8, 'dup_remove' : 1, 'post_blogurl_without' : '', 'srchby' : 'all', 'st' : 'date', 'where' : 'post'}
    d['query'] = self.query
    d['date_from'] = self.d_range0[i] #ex. '2020.03.01'
    d['date_to'] = self.d_range0[i]
    d['start'] = 1
    
  

    response = requests.get(base_url, params=d)
    soup = BeautifulSoup(response.text, 'lxml')

    #총 블로그 개수 확인
    tot = soup.select( '#main_pack > div.blog.section._blogBase._prs_blg > div > span' )[0] # 검색 결과 개수 가져오기
    
    if isinstance(tot , type(None)):
      return None #검색 결과가 아예 없으면 여기서 끝. 임시파일도 생성x. 뉴스는 있는데 네이버 뉴스만 있으면 빈 임시파일이 생성됨.
    else:
      tot = re.split(' / ', tot.text)[1][0:-1] # 1-10 / 629건 과 같은 형식에서 629만 가져오기. '/'로 쪼갠 다음, '건'을 지운다.
      tot = int(tot.replace(',','')) # 나눗셈을 하기 위해 자릿수 표시하는 ','를 지우고, string을 int로 변경
      
      self.count_NB[ self.d_range2[i] ] = tot #이 날의 뉴스 개수를 count_NC에 저장
      
      print('검색된 블로그 총 {}개'.format(tot)) # 블로그 개수 출력
      total_pages = math.ceil(tot/10) # 검색 결과 페이지 수  
      print('총', total_pages, '개 페이지')
    
      
  #2. 링크 추출

    sample_count = 0
    for j in range(total_pages):
      if (sample_count < self.sample) or self.sample ==0:
        d['start'] = str(j*10 + 1) # 1, 11, 21, 31, ...
        response = requests.get(base_url, params=d)
        soup = BeautifulSoup(response.text, 'lxml')
        
        litags = soup.select('ul.type01 > li') #ok
        for litag in litags:
            item = litag.select('dl > dt > a')[0]
            title = item.text
            url = item['href']   
            df = df.append(pd.DataFrame([[title, url]], columns=['title','url']), ignore_index=True)
            sample_count += 1
      else:
        break
        if j%10 ==0:
          print('{}th page'.format(j))
        
    #링크 다 가져온 후 결과 출력
    df['time'] = self.d_range1[i] #i가 맞음
    df['v1'] = np.arange(len(df.index))
    df.drop_duplicates(['url'])
    df = df.drop('v1', axis = 1)

    if self.sample > 0 and len(df.index) > self.sample:
      df = df[:self.sample]
      
    df.to_csv( self.NB_path[2] +'{}_url_{}_{}_{}.csv'.format('NB', self.query, self.d_range1[i], i),index=False) #i가 맞음
    print('총 {}개의 네이버 블로그 중 {}개의 네이버 블로그 주소를 가져옴'.format(tot, len(df.index)))
  
  
#########################################################################################################
  #네이버 블로그 본문 긁어오는 함수
  def getNBcontent(self, i):
    
    url = self.NB_links.iloc[i,1]
    print(url)
    df = pd.DataFrame( columns=['url', 'content'] ) 

    blog_url = 'https://m.' + url[8:]
    response = requests.get(blog_url)
    bs = BeautifulSoup(response.content, 'html5lib')

    #본문
    text_divs1 = bs.select('.se_textView > .se_textarea > span,p')
    text_divs2 = bs.select('.post_ct span')

    if len(text_divs1) > len(text_divs2):
        final_text_div = text_divs1
    else:
        final_text_div = text_divs2
    
    blog_content = ''
    for text in final_text_div:
        text = re.sub(r'(\<.+?\>)', ' ', str(text))
        if text not in blog_content:
            blog_content += text

    blog_content = re.sub(' +', ' ', str(blog_content))
    blog_content = blog_content.replace('\n', '')
    blog_content = blog_content.replace('\t', '')
    blog_content = blog_content.replace('\xa0', '')
    blog_content = blog_content.replace('본문 기타 기능 본문 폰트 크기 작게 보기 본문 폰트 크기 크게 보기 가 ', '')
    blog_content = blog_content.strip()

      
    df = df.append(pd.DataFrame([[blog_url, blog_content]], columns = ['url', 'content']), ignore_index=True)
      #print(i + 1, '번째 댓글 추출') #결과 알림
      
  
    df['time'] = self.NB_links.iloc[i,2]

    
    df['v1'] = np.arange(len(df.index))
    df.drop_duplicates(['content'])
    df = df.drop('v1', axis = 1)
    df.to_csv(self.NB_path[3]+'/{}_content_{}_xx_{}.csv'.format('NB', self.query, i),index=False)

#########################################################################################################
  
  def get_tweet(self, user_info):
    print("=== 설정된 트윗 수집 기간은 {} 에서 {} 까지 입니다 ===".format(self.d_range2[0], self.d_range2[-1]))
    print("=== 총 {}일 간의 데이터 수집 ===".format(len(self.d_range2)))
    ################################################################################
    # 특정 검색어가 포함된 트윗 검색하기 (quary search)

    # 수집 기간 맞추ll
    start_date = self.d_range2[0]
    end_date = (datetime.datetime.strptime(self.d_range2[-1], "%Y-%m-%d") + datetime.timedelta(days=1)).strftime("%Y-%m-%d") # setUntil이 끝을 포함하지 않으므로, day + 1

    # 트윗 수집 기준 정의
    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(self.query)\
                                            .setSince(start_date)\
                                            .setUntil(end_date)\
                                            .setMaxTweets(self.sample)

    # 수집 with GetOldTweet3
    print("수집 시작.. from {} to {}".format(self.d_range2[0], self.d_range2[-1]))
    start_time = time.time()

    tweet = got.manager.TweetManager.getTweets(tweetCriteria)

    print("수집 종료.. {0:0.2f} Minutes".format((time.time() - start_time)/60))
    print("=== Total num of tweets is {} ===".format(len(tweet)))
    ################################################################################
    # 원하는 변수 골라서 저장하기

    # initialize
    tweet_list = []

    for index in tweet: #위에서 가져온 정보 정리
        
        # 메타데이터 목록 
        username = index.username
        link = index.permalink 
        content = index.text
        tweet_date = index.date.strftime("%Y-%m-%d")
        tweet_time = index.date.strftime("%H:%M:%S")
        retweets = index.retweets
        favorites = index.favorites
        
        # ===beautifulsoap로 유저 정보 수집 시작 ===
        if not user_info:
          info_list = [tweet_date, tweet_time, username, content, link, retweets, favorites]
          tweet_list.append(info_list)
        else:
          try:
            personal_link = 'https://twitter.com/' + username
            response = requests.get(personal_link)
            bs_obj = BeautifulSoup(response.text, 'html.parser')  
            uls = bs_obj.find("ul", {"class": "ProfileNav-list"}).find_all("li")
            div = bs_obj.find("div", {"class": "ProfileHeaderCard-joinDate"}).find_all("span")[1]["title"]

            # 가입일, 전체 트윗 수, 팔로잉 수, 팔로워 수
            joined_date = div.split('-')[1].strip()
            num_tweets = uls[0].find("span", {"class": "ProfileNav-value"}).text.strip()
            num_following = uls[1].find("span", {"class": "ProfileNav-value"}).text.strip()
            num_follower = uls[2].find("span", {"class": "ProfileNav-value"}).text.strip()
          
          except AttributeError:
            print("=== Attribute error occurs at {} ===".format(link))
            print("link : {}".format(personal_link))   
            pass
          
          info_list = [tweet_date, tweet_time, username, content, link, retweets, favorites, joined_date, num_tweets, num_following, num_follower]# 결과 합치기
          tweet_list.append(info_list)
          # 휴식. 너무 자주 긁어오면 차단당함
          time.sleep(uniform(1,2))
        
          
      
      
     
    ################################################################################
    # 파일 저장하기
    if user_info:
      twitter_df = pd.DataFrame(tweet_list, columns = ["time", "time2", "user_name", "content", "URL", "retweet_counts", "favorite_counts","user_created", "user_tweets", "user_followings", "user_followers"])
    else:
      twitter_df = pd.DataFrame(tweet_list, columns = ["time", "time2", "user_name", "content", "URL", "retweet_counts", "favorite_counts"])
    # csv 파일 만들기
    twitter_df.to_csv(self.T_path + "sample_twitter_data_{}_to_{}.csv".format(self.d_range2[0], self.d_range2[-1]), index=False)
    self.tweet = twitter_df
    print("=== {} tweets are successfully saved ===".format(len(tweet_list)))

#####################################################
  def tokenizer(self, dtp):
    # 품사 태깅
    if dtp == "tweet":
      contents = self.tweet['content']
      mypath = self.T_path
    if dtp =='NC_title':
      contents = self.NC_links['title']
      mypath - self.NC_path[0]
    n = len(contents) 
    print(n, '원본 테이블')
    
    pos_so_far = []
    for content in contents:
      if type(content) == str:
        pos_so_far.append( konlpy.tag.Okt().pos(content) )
      else:
        pos_so_far.append([])

    print(len(pos_so_far), '품사 태깅')
    ################################################################################
    # 뺄 단어 리스트 만들기.
    stop_list_so_far = []
    #stopwords 뽑기
    for sentence in pos_so_far:
      for word in sentence:
        if word[1] not in ['Noun']:
          stop_list_so_far.append(word[0])
    stop_list = list(set(stop_list_so_far))
    with open('/content/drive/My Drive/Crawling/korean_stopwords.txt', 'r') as korstop:
      for word in korstop.readlines():
        stop_list.append(word.strip())

    print(len(stop_list), 'stopwords')
    ################################################################################
    # stopword 써서 토큰화

    token_so_far = []
    for content in contents:
      if type(content) == str:
        token_so_far.append( konlpy.tag.Okt().morphs(content) )
      else:
        token_so_far.append([])
    
    print(len(token_so_far), '토큰')
    token_new = []
    for line in token_so_far:
      new_line = []
      for word in line:
        if (len(word) > 1) & (word not in stop_list): #한 글자 단어와 stopword는 다 제거
          new_line.append(word)
      token_new.append(new_line)
    print(len(token_new), '최종')
    tokens = json.dumps(token_new)
    with open(mypath + 'token.json', 'w') as fileref:
      fileref.write(tokens)
    
    if dtp == "tweet":
      self.tweet_token = token_new
    if dtp == 'NC_title':
      self.NC_title_token = token_new
################################################################################
  #감성 점수 계산
  def scorer(self, dtp):
    with open('/content/drive/My Drive/Crawling/SentiWord_info.json', 'r') as fileref:
      dict_str = fileref.read() 
    sent_dict = json.loads(dict_str) #감성어 사전

    if dtp == 'tweet':
      my_words = self.tweet_token
      #with open( path + '/token.json', 'r') as fileref:
        #my_words = json.loads(fileref.read()) #토큰파일
      dfdf = self.tweet#댓글 파일
      path = self.T_path


    ################################################################################
    #점수 계산
    scorelist = df = pd.DataFrame(np.nan, index = np.arange(len(my_words)), columns=['pos', 'neu', 'neg', 'in_dict', 'out_dict'])
    counter = 0

    for i in range(len(my_words)):
      #print(i, '번째 텍스트에 점수 부여')
      sentence = my_words[i]
      scorelist.loc[i,['pos','neu','neg','in_dict','out_dict']] = 0
      
      score = {'pos':0, 'neu':0, 'neg':0, 'in_dict':0}
      for word in sentence:
        #print(word)
        counter = 0
        for sentword in sent_dict:
          if word in [ sentword['word'], sentword['word_root'] ] and counter == 0:
            counter = 1
            pol = int(sentword['polarity'] )
            if pol > 0:
              score['pos']+=1
            elif pol == 0:
              score['neu']+=1
            else:
              score['neg']+=1
      score['in_dict'] = sum(score.values())
      scorelist.loc[i, ['pos', 'neu', 'neg', 'in_dict', 'out_dict']] = [score['pos'], score['neu'], score['neg'], score['in_dict'], len(sentence) - score['in_dict']]
      ################################################################################
      # rate 계산
      scorelist[ 'pos_rate' ] = scorelist['pos'] / scorelist['in_dict']
      scorelist[ 'neg_rate' ] = scorelist['neg'] / scorelist['in_dict']
      ################################################################################
      #테이블 join
      final = dfdf.join(scorelist, how='left')
      final.to_csv(path + '/' + 'score.csv', index = False)
      ################################################################################
      #일별 합계
      score_per_day = final.groupby('time')[ ['pos', 'neu', 'neg', 'in_dict'] ].sum()
      score_per_day['pos_rate'] = score_per_day['pos'] / score_per_day['in_dict']
      score_per_day['neg_rate'] = score_per_day['neg'] / score_per_day['in_dict']
      score_per_day.to_csv(path + '/' + 'score_per_day.csv')
      ################################################################################
      ## 전체 합계
      tot_ratio= [ final['pos'].sum(), final['neg'].sum(), final['neu'].sum()]
      
      if dtp == 'tweet':
        self.tweet_score = final
        self.tweet_score_per_day = score_per_day
        self.tweet_tot_ratio = tot_ratio
################################################################################
#작업 내역 불러오기
  def set_n_iter(self, worklist, path):
    n_iter = range( len(worklist) )
      
    if self.retry:
      chkpoints = os.listdir( path )
      dones = []
      for chkpoint in chkpoints:
        dones.append( int(chkpoint.split('_')[4][:-4]) )
          
      if len(dones) > 0:
        print('링크 추출 작업 내역이 남아있음:' )
        #for i in dones:
          
         # print(worklist[ i ] )
        n_iter = list(  set(n_iter) - set( dones) )
        print('남은 작업 시작')
    work_left = len(n_iter) / len(worklist)
    if work_left == 1:
      print('작업 내역 없음')
    else:
      print('남은 작업 분량:', work_left)
    return n_iter

# 일별 뉴스 개수 가져오는 함수
def how_many_naver_news(query, s_date, e_date):
  dt_index = pd.date_range(start = s_date, end = e_date)
  dt_list = dt_index.strftime("%Y%m%d").tolist()
  
  how_many = {}
  for dt in dt_list:
    dtt = dt.replace('.', '')
    base_url = 'https://search.naver.com/search.naver'
    d = {'sort':1, 'photo':0, 'field':0, 'where':'news', 'reporter_article':'', 'pd':3, 'docid':'', 'refresh_start':0, 'mynews':0 }
    d['query'] = query
    d['ds'] = dt
    d['de'] = dt
    d['start'] = 1
    d['nso'] = 'so:dd,p:from{}to{},a:all'.format(dtt, dtt )
  
    response = requests.get(base_url, params=d)
    soup = BeautifulSoup(response.text, 'lxml')

    total_news = soup.find('div', 'title_desc') # 검색 결과 개수 가져오기
    total_news = re.split(' / ', total_news.text)[1][0:-1] # 1-10 / 629건 과 같은 형식에서 629만 가져오기. '/'로 쪼갠 다음, '건'을 지운다.
    total_news = int(total_news.replace(',','')) # 나눗셈을 하기 위해 자릿수 표시하는 ','를 지우고, string을 int로 변경
    how_many[dt] = total_news
  return how_many

shutil.rmtree('/content/drive/My Drive/Crawling/crawl_코로나_2020-04-25_2020-04-25')

oid = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=003&aid=0009828569'.split('&')[-2][4:]
aid = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=003&aid=0009828569'.split('&')[-1][4:]
n_url = 'https://n.news.naver.com/article/{}/{}'.format(oid, aid) 
url = 'https://n.news.naver.com/article/001/0011577882'
#url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=003&aid=0009828569'
response = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})
soup = BeautifulSoup(response.text, 'lxml') 
news_area = soup.select('div#dic_area')
news_area[0].text

oid = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=003&aid=0009828569'.split('&')[-2][4:]
aid = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=003&aid=0009828569'.split('&')[-1][4:]
n_url = 'https://n.news.naver.com/article/{}/{}'.format(oid, aid)



n_url

"""#센터장님"""

oid = test.NC_links.url[1].split('&')[-2][4:]
aid = test.NC_links.url[1].split('&')[-1][4:]

url = 'https://n.news.naver.com/article/{}/{}'.format(oid, aid)
print(url)

i = 2
url_desk = test.NC_links.loc[i, 'url']
oid = test.NC_links.url[1].split('&')[-2][4:]
aid = test.NC_links.url[1].split('&')[-1][4:]
url_mob = 'https://n.news.naver.com/article/{}/{}'.format(oid, aid)


response = requests.get(url_mob, headers={'User-Agent':'Mozilla/5.0'})
soup = BeautifulSoup(response.text, 'lxml') 
news_area = soup.select('div#dic_area')
  

content = news_area[0].text

news_area[0].text

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import konlpy
from konlpy.tag import Kkma
import math
import mglearn
import matplotlib.pyplot as plt
from wordcloud import WordCloud


#from matplotlib import font_manager, rc
#font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
#rc('font', family=font_name)

def dailygraphs(n_topic, date, dir):
    news = pd.read_csv(dir +'/NC_url_{}_{}.csv'.format(date, date))
    n = len(news)
    print('{}의 육군 뉴스 총 {}개'.format(date, n))
    titles = news.title
    
    titles_broken = []

    kkma = Kkma()
    for title in titles:
        titles_broken.append( kkma.morphs(title) )

    ' '.join( titles_broken[0] )

    titles_join = []
    for title_broken in titles_broken:
        titles_join.append(' '.join( title_broken))


    vect = CountVectorizer(min_df = 3, max_df = int(n*0.2))
    vect.fit(titles_join )
    print("어휘 사전의 크기:", len(vect.vocabulary_))
    #print('어휘 사전의 내용:', vect.vocabulary_)
    BOW = vect.transform(titles_join)

    lda = LatentDirichletAllocation(n_components = n_topic, learning_method = 'batch', max_iter = 25, random_state = 42)
    document_topics = lda.fit_transform(BOW)
    print('done')

    
   
    

    #각 토픽에서 가장 중요한 단어
    print('각 토픽에서 가장 중요한 단어')
    sorting = np.argsort(lda.components_, axis = 1)[:, ::-1]
    a, b = sorting.shape
    feature_names = np.array(vect.get_feature_names())
    
    mglearn.tools.print_topics(topics = range(n_topic), feature_names = feature_names, sorting = sorting, topics_per_chunk = 5, n_words = 10)
    
    
    #각 토픽의 전체 비중
    print('각 토픽의 전체 비중')
    topic_names = ['{:>2} '.format(i) + ' '.join(words) for i, words in enumerate(feature_names[sorting[:,:4]])]
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.barh( np.arange( n_topic ), np.sum(document_topics, axis=0) )
    ax.set_yticks(np.arange(n_topic))
    ax.set_yticklabels(topic_names)
    ax.invert_yaxis()
    ax.set_xlim(0, 60)
    #yax.set_tick_params(pad=130)
    plt.tight_layout()
    plt.savefig(dir + '/{}_topicgraph_{}_topics.png'.format(date, n_topic))
    
    #wordcloud
    for topic in range(n_topic):
        words_for_cloud = {}
        for word_idx in sorting[topic]:
            words_for_cloud[ feature_names[word_idx] ] = lda.components_[topic, word_idx]
            #wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf', background_color='white',colormap = "Accent_r", width=2000, height=1500).generate_from_frequencies(words_for_cloud)
            wordcloud = WordCloud(font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', background_color='white',colormap = "Accent_r", width=600, height=400).generate_from_frequencies(words_for_cloud)
            plt.imshow(wordcloud)
            plt.axis('off')
            plt.savefig(dir + '/{}_wordcloud_topic{}.png'.format(date, topic))

    with open(dir + '/top5 news_per_topic.txt', 'w') as f:
      for topic in range(n_topic):
    # 각 토픽별 비중 높은 문서 5개
        f.write('topic '+ str(topic))
        top = np.argsort(document_topics[:,topic])[::-1]
        for i in top[:5]:
            f.write('[{}] {}'.format(news.loc[i,'press'],  news.loc[i,'title']))



top5 = {}
with open(path + '/top5 news_per_topic.txt', 'w') as f:
    for topic in range(n_topic):
    # 각 토픽별 비중 높은 문서 5개
        top5['topic {}'.format(topic)] = []
        top = np.argsort(document_topics[:,topic])[::-1]
        for i in top[:5]:
            top5['topic {}'.format(topic)].append('[{}] {}'.format(news.loc[i,'press'],  news.loc[i,'title']))

with open(path + '/top5 news_per_topic.txt', 'w') as f:
    for topic in range(n_topic):
    # 각 토픽별 비중 높은 문서 5개
        f.write('topic '+ str(topic) + '\n')
        top = np.argsort(document_topics[:,topic])[::-1]
        for i in top[:5]:
            f.write('[{}] {}\n'.format(news.loc[i,'press'],  news.loc[i,'title']))

top10s = []
for topic in range(n_topic):
    top10 = []
    for word_idx in sorting[:20][topic]:
        top10.append( feature_names[word_idx] ) 
    top10s.append(top10)
print(top10s)



import plotly.graph_objects as go
from plotly.subplots import make_subplots

import pandas as pd
import re

df = pd.read_csv("https://raw.githubusercontent.com/plotly/datasets/master/Mining-BTC-180.csv")

for i, row in enumerate(df["Date"]):
    p = re.compile(" 00:00:00")
    datetime = p.split(df["Date"][i])[0]
    df.iloc[i, 1] = datetime

fig = make_subplots(
    rows=3, cols=1,
    shared_xaxes=True,
    vertical_spacing=0.03,
    specs=[[{"type": "table"}],
           [{"type": "scatter"}],
           [{"type": "scatter"}]]
)

fig.add_trace(
    go.Scatter(
        x=df["Date"],
        y=df["Mining-revenue-USD"],
        mode="lines",
        name="mining revenue"
    ),
    row=3, col=1
)

fig.add_trace(
    go.Scatter(
        x=df["Date"],
        y=df["Hash-rate"],
        mode="lines",
        name="hash-rate-TH/s"
    ),
    row=2, col=1
)

fig.add_trace(
    go.Table(
        header=dict(
            values = topic_names,
            font=dict(size=10),
            align="left"
        ),
        cells=dict(
            values=top10s,
            align = "left")
    ),
    row=1, col=1
)
fig.update_layout(
    height=1200,
    showlegend=False,
    title_text="{}의 육군 이슈".format(date),
)


fig.show()

from plotly.subplots import make_subplots
from skimage import data
img = data.chelsea()
fig = make_subplots(1, 2)
# We use go.Image because subplots require traces, whereas px functions return a figure
fig.add_trace(go.Image(z=img), 1, 1)
for channel, color in enumerate(['red', 'green', 'blue']):
    fig.add_trace(go.Histogram(x=img[..., channel].ravel(), opacity=0.5,
                               marker_color=color, name='%s channel' %color), 1, 2)
fig.update_layout(height=400)
fig.show()

"""# 자동화"""

schedule.clear()

코로나import schedule
import time

def job():
  print('작업')

schedule.every(10).seconds.do(job)

while True:
    schedule.run_pending()
    time.sleep(1)

schedule.clear(tag='test')

emoji = Crawler( "😠 OR 😡 lang:ko", '2019-04-20', '2019-10-20', dir = '/content/drive/My Drive/Crawling', sample = 10000)
emoji.get_tweet(user_info = False)

emoji = Crawler( "😠 OR 😡 lang:ko", '2019-10-21', '2020-04-21', dir = '/content/drive/My Drive/Crawling', sample = 10000)
emoji.get_tweet(user_info = False)

emoji = Crawler( "😄 OR 😊 OR ❤ lang:ko", '2019-04-20', '2019-10-20', dir = '/content/drive/My Drive/Crawling', sample = 10000)
emoji.get_tweet(user_info = False)

emoji = Crawler( "😄 OR 😊 OR ❤ lang:ko", '2019-10-21', '2020-04-21', dir = '/content/drive/My Drive/Crawling', sample = 10000)
emoji.get_tweet(user_info = False)

emoji = Crawler( "😠 코로나", '2020-01-20', '2020-04-20', dir = '/content/drive/My Drive/Crawling', sample = 50000)
emoji.get_tweet(user_info = False)

emoji = Crawler( "😠 ", '2020-01-20', '2020-04-20', dir = '/content/drive/My Drive/Crawling', sample = 50000)
emoji.get_tweet(user_info = False)

import datetime

def job():
  today = datetime.datetime.today().strftime("%Y-%m-%d")    # YYYYmmddHHMMSS 형태의 시간 출력
  test = Crawler('지코', today, today, dir = '/content/drive/My Drive/Crawling')
  test.get_tweet()
  #test.tokenizer('tweet')
  #test.scorer('tweet')

#schedule.every(1).minutes.do(job)
schedule.every().days.at("11:31").do(job).tag('crawling')
while True:
    schedule.run_pending()
    time.sleep(60)

from pprint import pprint

print("Job 확인")
pprint(schedule.jobs)

schedule.every().day.at("10:30").do(job2)

datetime.datetime.today()

emoji = Crawler( "코로나", '2020-04-24', '2020-04-24', dir = '/content/drive/My Drive/Crawling', sample = 1000)
emoji.get_tweet(user_info = True)

emoji = Crawler( "코로나", '2020-04-25', '2020-04-25', dir = '/content/drive/My Drive/Crawling', sample = 1000)
emoji.get_tweet(user_info = True)

"""# word2vec"""

#import tensorflow as tf
import numpy as np
import codecs

os.chdir("/content/drive/My Drive/Sentimental-Analysis-master/Word2Vec/Movie_rating_data") #bash의 cd와 동일. 디렉터리 설정

#텍스트 파일 줄별로 읽어오는 함수
def read_data(filename): 
    with open(filename, 'r',encoding='utf-8') as f:
        ## f.read().splitlines()는 \n이 뒤에 붙지 않는다는 점을 뺴면 f.readlines()와 같은 역할.
        data = [line.split('\t') for line in f.read().splitlines()] #결과물은 list        
        data = data[1:]   # header 제외 #    
    return data 
    
train_data = read_data('ratings_train.txt') 
test_data = read_data('ratings_test.txt') 

pos_tagger = Okt() 

def tokenize(doc):

    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]


## training Word2Vec model using skip-gram   
tokens = [tokenize(row[1]) for row in train_data]
model = gensim.models.Word2Vec(size=300,sg = 1, alpha=0.025,min_alpha=0.025, seed=1234)
model.build_vocab(tokens)
    
for epoch in range(30):
           
    model.train(tokens, total_examples = model.corpus_count,epochs = model.iter)
    model.alpha -= 0.002
    model.min_alpha = model.alpha
    
os.chdir("/content/drive/My Drive/Sentimental-Analysis-master/Word2Vec")    
model.save('Word2vec.model')
model.most_similar('팝콘/Noun',topn = 20)  ## topn = len(model.wv.vocab)

def get_url_naver_blog_for_pool(dt):  
  ulist = get_url_naver_blog(query, dt, dt)
  ulist['time'] = dt
  ulist['v1'] = np.arange(len(ulist.index))
  ulist.drop_duplicates(['url'])
  ulist.to_csv(path_temp_url+'/naver_blog_url{}_{}.csv'.format(query, dt),index=False)

  
#블로그 검색 결과에서 주소와 제목 가져오는 함수
def get_url_naver_blog(query, s_date, e_date):
  

  

  print('검색된 블로그 총 {}개'.format(tot)) # 블로그 개수 출력
  total_pages = math.ceil(tot/10) # 검색 결과 페이지 수  
  print('총', total_pages, '개 페이지')

  links = []
  titles = []
  df = pd.DataFrame( columns=['title', 'url'] ) 

  for i in range(total_pages): 
      #i+1번째 검색결과 페이지로 이동
      d['start'] = str(i*10 + 1) # 1, 11, 21, 31, ...
    
      #BeautifulSoup 시작
      response = requests.get(base_url, params=d)
      soup = BeautifulSoup(response.text, 'lxml')
      litags = soup.select('ul.type01 > li') #ok
      for litag in litags:
        item = litag.select('dl > dt > a')[0]
        title = item.text
        url = item['href']   
        df = df.append(pd.DataFrame([[title, url]], columns=['title','url']), ignore_index=True)
        ###################
  #링크 다 가져온 후 결과 출력
  print('총 {}개의 블로그 중 {}개의 블로그 주소를 가져옴.'.format(tot, len(df.index) ) )
  return(df)


#블로그 본문 긁어오기를 멀티프로세싱에 집어넣기 위한 함수
def get_blog_naver_for_pool(i):  
  title = all_url.iloc[i,0]
  url = all_url.iloc[i,1]
  dt = all_url.iloc[i,2]
  ulist = blog_getter_naver(url)
  ulist['title'] = title
  ulist['time'] = dt
  ulist.to_csv(path_temp_blog+'/naver_blog_{}_{}_{}.csv'.format(query, dt, i),index=False)

# 블로그 본문 내용 긁어오는 함수
def blog_getter_naver(url):

  df = pd.DataFrame( columns=['url', 'content'] ) 
  
  blog_url = 'https://m.' + url[8:]
  response = requests.get(blog_url)
  bs = BeautifulSoup(response.content, 'html5lib')

  #본문
  text_divs1 = bs.select('.se_textView > .se_textarea > span,p')
  text_divs2 = bs.select('.post_ct span')

  if len(text_divs1) > len(text_divs2):
    final_text_div = text_divs1
  else:
    final_text_div = text_divs2
    
  blog_content = ''
  for text in final_text_div:
    text = re.sub(r'(\<.+?\>)', ' ', str(text))
    if text not in blog_content:
      blog_content += text

  blog_content = re.sub(' +', ' ', str(blog_content))
  blog_content = blog_content.replace('\n', '')
  blog_content = blog_content.replace('\t', '')
  blog_content = blog_content.replace('\xa0', '')
  blog_content = blog_content.replace('본문 기타 기능 본문 폰트 크기 작게 보기 본문 폰트 크기 크게 보기 가 ', '')
  blog_content = blog_content.strip()

  df = df.append(pd.DataFrame([[blog_url, blog_content]], columns = ['url', 'content']), ignore_index=True)
  
  return(df)

"""# sentiment score

1. BOW를 만든다
2. 단어 리스트를 감성사전과 대조한다.
    1. 긍정이면 1
    2. 부정이면 -1
    3. 중립이면 0
3. 2를 가지고 단어 점수 벡터를 만든다.
4. BOW 행렬과 3을 곱하면 단어별 감성점수 벡터가 나온다.
5. BOW행렬을 sum해서 감성점수 분모를 만든다.
6. 4를 5로 나눈다.
"""

import json

def clean_text(text):
    cleaned_text = re.sub('[a-zA-z]','',text)
    cleaned_text = re.sub('[\{\}\[\]\/?.,;:|\)*~`!^\-_+<>@\#$%&\\\=\(\'\"\♥\♡\ㅋ\ㅠ\ㅜ\ㄱ\ㅎ\ㄲ\ㅡ]','',cleaned_text)
    return cleaned_text

import numpy as np

tweet = pd.read_csv( '/content/drive/My Drive/Crawling/다음주 보고/Tweet/total_tweet.csv' ,encoding='utf-8')

n = len(tweet)
contents = tweet.content

contents_broken = []

kkma = Kkma()
for content in contents:
    try:
        contents_broken.append( kkma.morphs(content) )
    except:
        contents_broken.append([])

contents_join = []
for content_broken in contents_broken:
    contents_join.append(' '.join( content_broken))

vect = CountVectorizer(min_df = 4, max_df = int(n*0.25))
vect.fit(contents_join )
print("어휘 사전의 크기:", len(vect.vocabulary_))
#print('어휘 사전의 내용:', vect.vocabulary_)
BOW = vect.transform(contents_join)
word_list = list(vect.vocabulary_)

with open('/content/drive/My Drive/Crawling/SentiWord_info.json', 'r') as fileref:
    dict_str = fileref.read() 
sent_dict = json.loads(dict_str) #감성어 사전


word_scores = pd.DataFrame(columns = [ 'word', 'polar'])
for i in range(len(word_list)):
    word = word_list[i]
    for sentword in sent_dict:
        if ( (word in sentword['word']) or (word in sentword['word_root']) ):
            word_scores.loc[i, 'word'] = word
            word_scores.loc[i, 'polar'] = int( sentword['polarity'] ) 
            print(word)
            break

word_list_pd = pd.DataFrame(word_list, columns = ['word'])
score_vec = pd.merge(word_list_pd, word_scores, how = 'left', on='word')


score_vec_fin = pd.DataFrame(columns = [ 'word', 'polar'])
for i in score_vec.index:
    print(i)
    word = score_vec.loc[i,'word']
    idx = vect.vocabulary_[word]
    score_vec_fin.loc[idx, 'polar'] = score_vec.loc[i,'polar']
    score_vec_fin.loc[idx, 'word'] = word
score_vec_fin.sort_index(inplace = True)

score_vec_comp = score_vec_fin.polar
score_vec_comp2 = score_vec_comp.to_numpy().reshape(-1,1)

scipy.sparse.save_npz('/content/drive/My Drive/Crawling/다음주 보고/BOW.npz', BOW)
np.save('/content/drive/My Drive/Crawling/다음주 보고/scorevec.npy',score_vec_comp2 )

scores_fin = np.dot(BOW, score_vec_comp2)

import scipy.sparse
BOW = scipy.sparse.load_npz('/content/drive/My Drive/Crawling/다음주 보고/BOW.npz')

score_vec_comp2  = np.load('/content/drive/My Drive/Crawling/다음주 보고/scorevec.npy', allow_pickle = True)

score_vec_comp3 = np.repeat(0, len(score_vec_comp2))
for i in range( len(score_vec_comp2) ):
    if not np.isnan( score_vec_comp2[i][0] ):
        score_vec_comp3[i] = score_vec_comp2[i][0]
score_vec_comp3 = score_vec_comp3.reshape(-1,1)

r = scipy.sparse.csr_matrix.dot(BOW, score_vec_comp3)
tot = BOW.sum(axis=1)

np.count_nonzero(score_vec_comp3 == 1)

np.count_nonzero(r > 1)

tweet = pd.read_csv( '/content/drive/My Drive/Crawling/다음주 보고/Tweet/total_tweet.csv' ,encoding='utf-8')
tweet['polar_sum'] = r
tweet['tot'] = tot
tweet['sent_score'] = tweet['polar_sum'] / tweet['tot']

tweet.sort_values(by = 'sent_score')[:10].to_csv('bad.csv')

tweet.sort_values(by = 'sent_score')[:10]

tweet.sort_values(by = 'sent_score', ascending = False)[6:16].to_csv('good.csv')

# Commented out IPython magic to ensure Python compatibility.
lda = LatentDirichletAllocation(n_components = n_topic, learning_method = 'batch', max_iter = 25, random_state = 42)
document_topics = lda.fit_transform(BOW)
print('done')

#각 토픽에서 가장 중요한 단어
print('각 토픽에서 가장 중요한 단어')
sorting = np.argsort(lda.components_, axis = 1)[:, ::-1]
a, b = sorting.shape
feature_names = np.array(vect.get_feature_names())
    
mglearn.tools.print_topics(topics = range(n_topic), feature_names = feature_names, sorting = sorting, topics_per_chunk = 5, n_words = 10)


#각 토픽의 전체 비중
print('각 토픽의 전체 비중')
topic_names = ['{:>2} '.format(i) + ' '.join(words) for i, words in enumerate(feature_names[sorting[:,:4]])]
fig, ax = plt.subplots(figsize=(8, 8))
ax.barh( np.arange( n_topic ), np.sum(document_topics, axis=0) )
ax.set_yticks(np.arange(n_topic))
ax.set_yticklabels(topic_names)
ax.invert_yaxis()
ax.set_xlim(0, 60)
#yax.set_tick_params(pad=130)
plt.tight_layout()
plt.savefig(path + '/{}_topicgraph_{}_topics.png'.format(date, n_topic))

#wordcloud
for topic in range(n_topic):
    words_for_cloud = {}
    for word_idx in sorting[topic]:
        words_for_cloud[ feature_names[word_idx] ] = lda.components_[topic, word_idx]
        #wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf', background_color='white',colormap = "Accent_r", width=2000, height=1500).generate_from_frequencies(words_for_cloud)
    wordcloud = WordCloud(font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', background_color='white',colormap = "Accent_r",  width=600, height=400).generate_from_frequencies(words_for_cloud)
    array = wordcloud.to_array()
#     %matplotlib inline
    fig = plt.figure(figsize=(10, 10))
    plt.axis('off')
    plt.imshow(array, interpolation="bilinear")
    

    #plt.imshow(wordcloud)
    plt.savefig(path + '/{}_wordcloud_topic{}.png'.format(date, topic))
    print("{}th topic's cloud".format(topic))

top5 = {}
with open(path + '/top5 news_per_topic.txt', 'w') as f:
    for topic in range(n_topic):
    # 각 토픽별 비중 높은 문서 5개
        top5['topic {}'.format(topic)] = []
        top = np.argsort(document_topics[:,topic])[::-1]
        for i in top[:5]:
            top5['topic {}'.format(topic)].append('[{}] {}'.format(news.loc[i,'press'],  news.loc[i,'title']))

"""# Plotly"""

for i in sorting[1][:10]:
    print(i)

sorting = np.argsort(lda.components_, axis = 1)[:, ::-1]

sorting[0][1]

n_topic = 4
date = '20200429'
path = '/content/drive/My Drive/센터장'

news = pd.read_csv(path +'/NC_url_{}_{}.csv'.format(date, date))
n = len(news)
print('{}의 육군 뉴스 총 {}개'.format(date, n))
titles = news.title

titles_broken = []

kkma = Kkma()
for title in titles:
    titles_broken.append( kkma.morphs(title) )
    #' '.join( titles_broken[0] )

titles_join = []
for title_broken in titles_broken:
    titles_join.append(' '.join( title_broken))

vect = CountVectorizer(min_df = 3, max_df = int(n*0.2))
vect.fit(titles_join )
print("어휘 사전의 크기:", len(vect.vocabulary_))
#print('어휘 사전의 내용:', vect.vocabulary_)
BOW = vect.transform(titles_join)


lda = LatentDirichletAllocation(n_components = n_topic, learning_method = 'batch', max_iter = 25, random_state = 42)
document_topics = lda.fit_transform(BOW)
#document_topics는 row는 문서, column은 주제.

print('done')

#각 토픽에서 가장 중요한 단어
print('각 토픽에서 가장 중요한 단어')
sorting = np.argsort(lda.components_, axis = 1)[:, ::-1] 
                    #row는 주제, column은 단어
                    #[:, ::-1]는 가로 방향으로 뒤집는 기능. 단어 순서가 역순이었나 봄
a, b = sorting.shape
feature_names = np.array(vect.get_feature_names())
    
#mglearn.tools.print_topics(topics = range(n_topic), feature_names = feature_names, sorting = sorting, topics_per_chunk = 5, n_words = 10)
nt, nw = sorting.shape
top10 = []
for topic in range(nt):
    top10.append( [] )
    for i in sorting[topic][:10]:
        top10[topic].append(feature_names[i])

#각 토픽의 전체 비중
print('각 토픽의 전체 비중')
topic_names = ['{:>2} '.format(i) + ' '.join(words) for i, words in enumerate(feature_names[sorting[:,:4]])]
fig, ax = plt.subplots(figsize=(8, 8))
ax.barh( np.arange( n_topic ), np.sum(document_topics, axis=0) )
ax.set_yticks(np.arange(n_topic))
ax.set_yticklabels(topic_names)
ax.invert_yaxis()
ax.set_xlim(0, 60)
#yax.set_tick_params(pad=130)
plt.tight_layout()
plt.savefig(path + '/{}_topicgraph_{}_topics.png'.format(date, n_topic))

#wordcloud
clouds = []
for topic in range(n_topic):
    words_for_cloud = {}
    for word_idx in sorting[topic]:
        words_for_cloud[ feature_names[word_idx] ] = lda.components_[topic, word_idx]
        #wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf', background_color='white',colormap = "Accent_r", width=2000, height=1500).generate_from_frequencies(words_for_cloud)
    wordcloud = WordCloud(font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', background_color='white',colormap = "Accent_r",  width=600, height=400).generate_from_frequencies(words_for_cloud)
    clouds.append( wordcloud.to_array() )
    #%matplotlib inline
    ##fig = plt.figure(figsize=(10, 10))
    #plt.axis('off')
    #plt.imshow(array, interpolation="bilinear")
    

    #plt.imshow(wordcloud)
    #plt.savefig(path + '/{}_wordcloud_topic{}.png'.format(date, topic))
    #print("{}th topic's cloud".format(topic))

top5 = {}
with open(path + '/top5 news_per_topic.txt', 'w') as f:
    for topic in range(n_topic):
    # 각 토픽별 비중 높은 문서 5개
        top5['topic {}'.format(topic)] = []
        top = np.argsort(document_topics[:,topic])[::-1]
        for i in top[:5]:
            top5['topic {}'.format(topic)].append('[{}] {}'.format(news.loc[i,'press'],  news.loc[i,'title']))

fig.add_trace(
    
    go.Bar(
    y=['주제1', '주제2', '주제3'],
    x=[20, 14, 23],
    name='긍정',
    orientation='h',
    marker=dict(
        color='rgba(58, 71, 80, 0.6)',
        line=dict(color='rgba(58, 71, 80, 1.0)', width=3)
    )
)
,row = 1, col = 1

)

fig.add_trace(go.Bar(
     y=['주제1', '주제2', '주제3'],
    x=[12, 18, 29],
    name='중립',
    orientation='h',
    marker=dict(
        color='rgba(71, 58, 131, 0.8)',
        line=dict(color='rgba(71, 58, 131, 0.8)', width=3) 
    )
)
,row = 1, col = 1
)

fig.add_trace(go.Bar(
     y=['주제1', '주제2', '주제3'],
    x=[12, 18, 29],
    name='부정',
    orientation='h',
    marker=dict(
        color='rgba(246, 78, 139, 0.6)',
        line=dict(color='rgba(246, 78, 139, 1.0)', width=3)  
    )
),row = 1, col = 1
)

fig.update_layout(barmode='stack')

annotations = []

for yd, xd in zip(y_data, x_data):
    # labeling the y-axis
    annotations.append(dict(xref='paper', yref='y',
                            x=0.14, y=yd,
                            xanchor='right',
                            text=str(yd),
                            font=dict(family='Arial', size=14,
                                      color='rgb(67, 67, 67)'),
                            showarrow=False, align='right'))
    # labeling the first percentage of each bar (x_axis)
    annotations.append(dict(xref='x', yref='y',
                            x=xd[0] / 2, y=yd,
                            text=str(xd[0]) + '%',
                            font=dict(family='Arial', size=14,
                                      color='rgb(248, 248, 255)'),
                            showarrow=False))
    # labeling the first Likert scale (on the top)
    if yd == y_data[-1]:
        annotations.append(dict(xref='x', yref='paper',
                                x=xd[0] / 2, y=1.1,
                                text=top_labels[0],
                                font=dict(family='Arial', size=14,
                                          color='rgb(67, 67, 67)'),
                                showarrow=False))
    space = xd[0]
    for i in range(1, len(xd)):
            # labeling the rest of percentages for each bar (x_axis)
            annotations.append(dict(xref='x', yref='y',
                                    x=space + (xd[i]/2), y=yd,
                                    text=str(xd[i]) + '%',
                                    font=dict(family='Arial', size=14,
                                              color='rgb(248, 248, 255)'),
                                    showarrow=False))
            # labeling the Likert scale
            if yd == y_data[-1]:
                annotations.append(dict(xref='x', yref='paper',
                                        x=space + (xd[i]/2), y=1.1,
                                        text=top_labels[i],
                                        font=dict(family='Arial', size=14,
                                                  color='rgb(67, 67, 67)'),
                                        showarrow=False))
            space += xd[i]

fig.update_layout(annotations=annotations)

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly
import pandas as pd

import plotly.express as px
df1 = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')
df2 = px.data.tips()


n_row = 2
n_col = 4
h_space = 0.4
v_space = 0.6
# Initialize figure with subplots
fig = make_subplots(
    rows = n_row, cols = n_col,
    horizontal_spacing = h_space / n_col,
    vertical_spacing = v_space / n_row,

    specs=[
           [ {"type": "bar"}, {"type": "table"}, {"type": "table"} , None],
           [  {'type': 'scatter', 'colspan':2},None, {"type": "pie"},    {}]
           ]
    #,subplot_titles=("First Subplot","Second Subplot", "Third Subplot", "First Subplot","Second Subplot", "Third Subplot", "First Subplot","Second Subplot")
    )

#################################################################################
# 1. LDA 주제별 막대그래프


top_labels = ['부정', '긍정', '중립']

colors = ['rgba(38, 24, 74, 0.8)', 'rgba(71, 58, 131, 0.8)',
          'rgba(122, 120, 168, 0.8)']

x_data = [[21, 30, 21],
          [24, 31, 19],
          [27, 26, 23],
          [29, 24, 15]]

y_data = []
for i in range(n_topic):
    y_data.append('topic {}'.format(i))


for i in range(0, len(x_data[0])):
    for xd, yd in zip(x_data, y_data):
        fig.add_trace(go.Bar(
            x=[xd[i]], y=[yd],
            orientation='h',
            marker=dict(
                color=colors[i],
                line=dict(color='rgb(248, 248, 249)', width=1)
            ), xaxis = 'x{}'.format(i+1), yaxis = 'y{}'.format(i+1)
        ), row = 1, col = 1 )


fig.update_layout(
    barmode='stack',
    paper_bgcolor='rgb(248, 248, 255)',
    plot_bgcolor='rgb(248, 248, 255)',
    margin=dict(l=120, r=10, t=140, b=80),
    showlegend=False)

#################################################################################
#2. 토픽별 핵심기사 5개

for i in range(n_topic):
    fig.add_trace(
        go.Table(
            header=dict(
                values=['Topic {}'.format(i)],
                font=dict(size=10),
                align="left"
            ),
            cells=dict(
                values=[ top5[ 'topic {}'.format(i) ] ],
                align = "left")
        ),
        row=1, col=2
    )


#################################################################################
#3. 토픽별 핵심어휘 10개
for i in range(n_topic):
    fig.add_trace(
        go.Table(
            header=dict(
                values=['Topic {}'.format(i)],
                font=dict(size=10),
                align="left"
            ),
            cells=dict(
                values=[ top10[i] ],
                align = "left")
        ),
        row=1, col=3
    )
###############################################################

# Add locations bar chart
fig.add_trace(
    #go.Bar(x=freq["x"][0:10],y=freq["Country"][0:10], marker=dict(color="crimson"), showlegend=False),
    go.Scatter(
    x = df1['Date'],
    y = df1['mavg']
),row=2, col=1
)

fig.add_trace(
    #go.Bar(x=freq["x"][0:10],y=freq["Country"][0:10], marker=dict(color="crimson"), showlegend=False),
    go.Scatter(
    x = df1['Date'],
    y = df1['AAPL.High']
),row=2, col=1
)

fig.update_xaxes(
    rangeslider_visible=True,
    rangeselector=dict(
        buttons=list([
            dict(count=1, label="1m", step="month", stepmode="backward"),
            dict(count=6, label="6m", step="month", stepmode="backward"),
            dict(count=1, label="YTD", step="year", stepmode="todate"),
            dict(count=1, label="1y", step="year", stepmode="backward"),
            dict(step="all")
        ])
    ),row=2, col=1
)
#################################################################################
#오늘의 긍정 부정 파이차트

labels = ['Oxygen','Hydrogen','Carbon_Dioxide','Nitrogen']
values = [4500, 2500, 1053, 500]

fig.add_trace(
   go.Pie(labels=labels, values=values),
    row=2, col=3
)

#################################################################################
##워드클라우드

for i in range(n_topic):
    fig.add_trace(
        go.Image(z = clouds[i]),
        row=2, col=4
    )

#fig.add_trace(
#   go.Image(z = array),
#    row=2, col=4
#)


#button booleans
bool_for_buttons = []
for i in range(n_topic):
    bool_for_buttons.append( list( np.repeat(True, n_topic*3) ) )
    bools= list( np.repeat(False, n_topic ) )
    bools[i] = True
    bool_for_buttons[i] = bool_for_buttons[i] + bools   +   bools +  list( np.repeat(True, 4) ) + bools
                                                #핵심기사    #핵심어휘                       #워드클라우드
dict_list = []
for i in range(n_topic):
    dict_list.append(
                        dict(label="topic {}".format(i),
                            method="update",
                            args=[
                                { "visible": bool_for_buttons[i] }
                                ])
    )


fig.update_layout(
    updatemenus=[
        dict(
            type="buttons",
            direction="right",
            active=0,
            x=0.42,
            y=1.03,
            buttons= dict_list
        )
    ])




# Set theme, margin, and annotation in layout
fig.update_layout(
    template="ggplot2",
    margin=dict(r=10, t=25, b=40, l=60),
    font=dict(
    family="NanumBarunGothic",
    size=16
    )
)

fig.show()
plotly.offline.plot(fig, filename = 'filename.html', auto_open=False)

"""#  윈도우에서 댓글 크롤링"""

import selenium
from selenium import webdriver  # selenium 프레임 워크에서 webdriver 가져오기
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

url = 'https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=001&aid=0011592887&isYeonhapFlash=Y&rc=N'
def getrep(url):
    browser = webdriver.Chrome('C:/Users/armybigdata/Downloads/chromedriver_win32/chromedriver.exe',options=chrome_options) #put this line inside the function def, or chrome winodws keeps opening
    rep_url = '{}&m_view=1&includeAllCount=true&m_url=%2Fcomment%2Fall.nhn%3FserviceId%3Dnews%26gno%3Dnews417%2C0000512678%26sort%3Dlikability'.format(url)    
    browser.implicitly_wait(4) #웹 드라이버
    browser.get(rep_url)
    #더보기 계속 클릭하기
    while True:
        try:
            see_more_button = browser.find_element_by_css_selector('.u_cbox_page_more')
            see_more_button.click()        
            time.sleep(1)
        except:
            break
        #댓글추출
    contents = browser.find_elements_by_css_selector('span.u_cbox_contents')
    for content in contents[:10]:
       print(content.text)

#contents 

print('a')

#14:56

import selenium
from selenium import webdriver  # selenium 프레임 워크에서 webdriver 가져오기
from bs4 import BeautifulSoup


chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

url = 'https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=001&aid=0011592887&isYeonhapFlash=Y&rc=N'

browser = webdriver.Chrome('C:/Users/armybigdata/Downloads/chromedriver_win32/chromedriver.exe',options=chrome_options) #put this line inside the function def, or chrome winodws keeps opening
rep_url = '{}&m_view=1&includeAllCount=true&m_url=%2Fcomment%2Fall.nhn%3FserviceId%3Dnews%26gno%3Dnews417%2C0000512678%26sort%3Dlikability'.format(url)    
browser.implicitly_wait(4) #웹 드라이버
browser.get(rep_url)
    #더보기 계속 클릭하기

while True:
    try:
        see_more_button = browser.find_element_by_css_selector('.u_cbox_page_more')
        see_more_button.click()        
        time.sleep(1)
    except:
        break
        #댓글추출
html = browser.page_source
soup = BeautifulSoup(html, 'lxml')

litags = soup.select('div.u_cbox_comment_box > div.u_cbox_area')
#댓글
litags[1].select('div.u_cbox_text_wrap > span.u_cbox_contents')[0].text
#추천
litags[1].select('div.u_cbox_tool > div.u_cbox_recomm_set > a')[0].select('em.u_cbox_cnt_recomm')[0].text
#비추천
litags[1].select('div.u_cbox_tool > div.u_cbox_recomm_set > a')[1].select('em.u_cbox_cnt_unrecomm')[0].text
#댓글작성날짜
litags[1].select('div.u_cbox_info_base > span.u_cbox_date')[0].text
#답글수
litags[1].select('div.u_cbox_tool > a.u_cbox_btn_reply > span.u_cbox_reply_cnt')[0].text



"""# 텐서플로"""

from google.colab import drive
drive.mount('/content/drive')

!pip uninstall tensorflow
!pip install tensorflow-gpu==1.15.2

# -*- coding: utf-8 -*-
"""
Created on Mon May 21 16:14:45 2018

@author: jbk48
"""
import sys
sys.path.append('/content/drive/My Drive/SA/Bidirectional_LSTM')
import time
import os
import tensorflow as tf
import numpy as np
import Bi_LSTM
import Word2Vec
import pandas as pd


config = tf.ConfigProto()
config.gpu_options.allow_growth = True

W2V = Word2Vec.Word2Vec()
os.chdir("/content/drive/My Drive/SA")
train_data = W2V.read_data("./Word2Vec/Movie_rating_data/ratings_train.txt")
test_data = W2V.read_data("./Word2Vec/Movie_rating_data/ratings_test.txt")

## tokenize the data we have
print("Tokenize Start!\nCould take minutes...")
train_tokens = [[W2V.tokenize(row[1]),int(row[2])] for row in train_data if W2V.tokenize(row[1]) != []]
train_tokens = np.array(train_tokens)
test_tokens = [[W2V.tokenize(row[1]),int(row[2])] for row in test_data if W2V.tokenize(row[1]) != []]
test_tokens = np.array(test_tokens)

print("Tokenize Done!")

train_X = train_tokens[:,0]
train_Y = train_tokens[:,1]
test_X = test_tokens[:,0]
test_Y = test_tokens[:,1]

train_Y_ = W2V.One_hot(train_Y)  ## Convert to One-hot
train_X_ = W2V.Convert2Vec("./Word2Vec/Word2vec.model",train_X)  ## import word2vec model where you have trained before
test_Y_ = W2V.One_hot(test_Y)  ## Convert to One-hot
test_X_ = W2V.Convert2Vec("./Word2Vec/Word2vec.model",test_X)  ## import word2vec model where you have trained before


Batch_size = 32
Total_size = len(train_X)
Vector_size = 300
train_seq_length = [len(x) for x in train_X]
test_seq_length = [len(x) for x in test_X]
Maxseq_length = max(train_seq_length) ## 95
learning_rate = 0.001
lstm_units = 128
num_class = 2
training_epochs = 4


with tf.device("/gpu:0"):
    X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size], name = 'X')
    Y = tf.placeholder(tf.float32, shape = [None, num_class], name = 'Y')
    seq_len = tf.placeholder(tf.int32, shape = [None])
    keep_prob = tf.placeholder(tf.float32, shape = None)

    BiLSTM = Bi_LSTM.Bi_LSTM(lstm_units, num_class, keep_prob)

    with tf.variable_scope("loss", reuse = tf.AUTO_REUSE):
        logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)
        loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate)

    prediction = tf.nn.softmax(logits)
    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))


init = tf.global_variables_initializer()

total_batch = int(len(train_X) / Batch_size)
test_batch = int(len(test_X) / Batch_size)

print("Start training!")

modelName = "./Bidirectional_LSTM/BiLSTM_model.ckpt"
saver = tf.train.Saver()

train_acc = []
train_loss = []
test_acc = []
test_loss = []


with tf.Session(config = config) as sess:

    start_time = time.time()
    sess.run(init)
    train_writer = tf.summary.FileWriter('./Bidirectional_LSTM', sess.graph)
    merged = BiLSTM.graph_build()
    
    for epoch in range(training_epochs):

        avg_acc, avg_loss = 0. , 0.
        mask = np.random.permutation(len(train_X_))
        train_X_ = train_X_[mask]
        train_Y_ = train_Y_[mask]
        
        for step in range(total_batch):

            train_batch_X = train_X_[step*Batch_size : step*Batch_size+Batch_size]
            train_batch_Y = train_Y_[step*Batch_size : step*Batch_size+Batch_size]
            batch_seq_length = train_seq_length[step*Batch_size : step*Batch_size+Batch_size]
            
            train_batch_X = W2V.Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)
            
            sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})
            # Compute average loss
            loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length,
                                              keep_prob : 0.75})
            avg_loss += loss_ / total_batch
            
            acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length,
                                                 keep_prob : 0.75})
            avg_acc += acc / total_batch
            print("epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}".format(epoch+1, step+1, loss_, acc))
   
        summary = sess.run(merged, feed_dict = {BiLSTM.loss : avg_loss, BiLSTM.acc : avg_acc})       
        train_writer.add_summary(summary, epoch)
    
        t_avg_acc, t_avg_loss = 0. , 0.
        print("Test cases, could take few minutes")
        for step in range(test_batch):
            
            test_batch_X = test_X_[step*Batch_size : step*Batch_size+Batch_size]
            test_batch_Y = test_Y_[step*Batch_size : step*Batch_size+Batch_size]
            batch_seq_length = test_seq_length[step*Batch_size : step*Batch_size+Batch_size]
            
            test_batch_X = W2V.Zero_padding(test_batch_X, Batch_size, Maxseq_length, Vector_size)
            
            # Compute average loss
            loss2 = sess.run(loss, feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length,
                                              keep_prob : 1.0})
            t_avg_loss += loss2 / test_batch
            
            t_acc = sess.run(accuracy , feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length,
                                                   keep_prob : 1.0})
            t_avg_acc += t_acc / test_batch

        print("<Train> Loss = {:.6f} Accuracy = {:.6f}".format(avg_loss, avg_acc))
        print("<Test> Loss = {:.6f} Accuracy = {:.6f}".format(t_avg_loss, t_avg_acc))
        train_loss.append(avg_loss)
        train_acc.append(avg_acc)
        test_loss.append(t_avg_loss)
        test_acc.append(t_avg_acc)

    train_loss = pd.DataFrame({"train_loss":train_loss})
    train_acc = pd.DataFrame({"train_acc":train_acc})
    test_loss = pd.DataFrame({"test_loss":test_loss})
    test_acc = pd.DataFrame({"test_acc":test_acc})
    df = pd.concat([train_loss,train_acc,test_loss,test_acc], axis = 1)
    df.to_csv("./Bidirectional_LSTM/loss_accuracy.csv", sep =",", index=False)
    
    train_writer.close()
    duration = time.time() - start_time
    minute = int(duration / 60)
    second = int(duration) % 60
    print("%dminutes %dseconds" % (minute,second))
    save_path = saver.save(sess, modelName)
    
    print ('save_path',save_path)